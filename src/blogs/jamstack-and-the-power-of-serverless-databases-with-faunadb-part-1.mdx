---
title: Jamstack and the power of serverless databases with FaunaDB. Part 1
published: false
category: Tutorial
author: Richard Haines
tags: [gatsby, theme-ui, netlify functions, faunadb, serverless, tutorial]
pin: false
---

In this three part series we will create a Jamstack website powered by Gatsby,
Netlify Functions, Apollo and FaunaDB. Our site will use the
[Harry Potter API](https://www.potterapi.com/) for its data that will be stored
in a [FaunaDB](https://fauna.com/) database. The data will be accessed using
serverless functions and [Apollo](https://www.apollographql.com/docs/). Finally
we will display our data in a [Gatsby](https://www.gatsbyjs.org/) site styled
using [Theme-ui](https://theme-ui.com/).

In this first part we will focus on what these technologies are and why, as
frontend developers, we should be leveraging them. We will then begin our
project and create our schema.

**The Jamstack**

Jamstack is a term often used to describe sites that are served as static assets
to a [CDN](https://www.cloudflare.com/learning/cdn/what-is-a-cdn/), of course
this is nothing new, anyone who has made a simple site with HTML and CSS and
published it has served a static site. To walk away thinking that the only
purpose of Jamstack sites are to serve static files would be doing it a great
injustice and miss some of the awesome things this "new" way of building web
apps provides.

A few of the benefits of going Jamstack

- High security and more secure. Fewer points of attack due to static files and
  external APIs served over CDN
- Cheaper hosting and easier scalability with serverless functions
- Fast! Pre-built assets served from a CDN instead of a server

A popular way of storing the data your site requires, apart from as markdown
files, is the use of a headless CMS (Content Management System). These CMSs have
adopted the term headless as they don't come with their own frontend that
displays the data stored, like Wordpress for example. Instead they are headless,
they have no frontend.

A headless CMS can be set up so that once a change to the data is made in the
CMS a new build is triggered via a webhook (just one way of doing it, you could
trigger rebuilds other ways) and the site will be deployed again with the new
data.

As an example we could have some images stored in our CMS that are pulled into
our site via a graphql query and shown on our site. If we wanted to change one
of our images we could do so via our CMS which would then trigger a new build on
publish and the new image would then be visible on our site.

There are many great options to choose from when considering which CMS to use:

- Netlify CMS
- Contenful
- Sanity.io
- Tina CMS
- Butter CMS

The potential list is so long i will point you in the direction of a great site
that lists most of them [headlesscms.org](https://headlesscms.org/)!

For more information and a great overview of what the Jamstack is and some more
of its benefits i recommend checking out [jamstack.org](https://jamstack.org/).

Just because our site is served as static assets, that doesn't mean we cant work
in a dynamic way and have the benefits of dynamic data! We wont be diving deep
into all of its benefits, but we will be looking at how we can take our static
site and make it dynamic by way of taking a serverless approach to handling our
data through AWS Lambda functions, which we will use via Netlify and FaunaDB.

**Serverless**

Back in the old days, long long ago before we spread our stack with jam, we had
a website that was a combination of HTML markup, CSS styling and JavaScript. Our
website gave our user data to access and manipulate and our data was stored in a
database which was hosted on a server. If we hosted this database ourselves we
were responsible for keeping it going and maintaining it and all of its stored
data. Our database could hold only a certain amount of data which meant that if
we were lucky enough to get a lot of traffic it would soon struggle to handle
all of the requests coming its way and so our end users might experience some
downtime or no data at all.

If we paid for a hosted server then we were paying for the up time even when no
requests were being sent.

To counter these issues serverless computing was introduced. Now, lets cut
through all the magic this might imply and simply state that serverless still
involves servers, the big difference is that they are hosted in the cloud and
execute some code for us.

Providing the requested resources as a simple function they only run when that
request is made. This means that we are only charged for the resources and time
the code is running for. With this approach we have done away with the need to
pay a server provider for constant up time, which is one of the big plus points
of going serverless.

Being able to scale up and down is also a major benefit of using serverless
functions to interact with our data stores. In a nutshell this means that as
multiple requests come in via our serverless functions, our cloud provider can
create multiple instances of the same function to handle those requests and run
them in parallel. One downside to this is the concept of cold starts where
because our functions are spun up on demand they need a small amount of time to
start up which can delay our response. However, once up if multiple requests are
received our serverless functions will stay open to requests and handle them
before closing down again.

**FaunaDB**

FaunaDB is a global serverless database that has native graphql support, is
multi tenancy which allows us to have nested databases and is low latency from
any location. Its also one of the only serverless databases to follow the
[ACID transactions](https://en.wikipedia.org/wiki/ACID) which guarantee
consistent reads and writes to the database.

Fauna also provides us with a High Availability solution with each server
globally located containing a partition of our database, replicating our data
asynchronously with each request with a copy of our database or the transaction
made.

Some of the benefits to using Fauna can be summarized as:

- Transactional
- Multi-document
- Geo-distributed

In short, Fauna frees the developer from worry about single or multi-document
solutions. Guarantees consistent data without burdening the developer on how to
model their system to avoid consistency issues. To get a good overview of how
Fauna does this see this
[blog post](https://fauna.com/blog/consistency-without-clocks-faunadb-transaction-protocol)
about the FaunaDB distributed transaction protocol.

There are a few other alternatives that one could choose instead of using Fauna
such as:

- Firebase
- Cassandra
- MongoDB

But these options don't give us the ACID guarantees that Fauna does,
compromising scaling.

**ACID**

- **Atomic** - all transactions are a single unit of truth, either they all pass
  or none. If we have multiple transactions in the same request then either both
  are good or neither are, one cannot fail and the other succeed.
- **Consistent** - A transaction can only bring the database from one valid
  state to another, that is, any data written to the database must follow the
  rules set out by the database, this ensures that all transactions are legal.
- **Isolation** - When a transaction is made or created, concurrent transactions
  leave the state of the database the same as is they would be if each request
  was made sequentially.
- **Durability** - Any transaction that is made and committed to the database is
  persisted in the the database, regardless of down time of the system or
  failure.

Now that we have a good overview of the stack we will be using lets get to the
code!

**Setup project**

We'll create a new folder to house our project, initialize it with yarn and add
some files and folders to that we will be working with throughout.

At the projects root create a functions folder with a nested graphql folder. In
that folder we will create three files, our graphql schema which we will import
into Fauna, our serverless function which will live in graphql.js and create the
link to and use the schema from Fauna and our database connection to Fauna.

```bash
    mkdir harry-potter
    cd harry-potter
    yarn init- y
    mkdir src/pages/
    cd src/pages && touch index.js
    mkdir src/components
    touch gatsby-config.js
    touch gatsby-browser.js
    touch gatsby-ssr.js
    touch .gitignore

    mkdir functions/graphql
    cd functions/graphql && touch schema.gql graphql.js db-connection.js
```

We'll also need to add some packages.

```bash
    yarn add gatsby react react-dom theme-ui gatsby-plugin-theme-ui faunadb isomorphic-fetch dotenv
```

Add the following to your newly created .gitignore file:

```bash
    .netlify
    node_modules
    .cache
    public
```

**Serverless setup**

Lets begin with our schema. We are going to take advantage of an awesome feature
of Fauna. By creating our schema and importing it into Fauna we are letting it
take care of a lot of code for us by auto creating all the classes, indexes and
possible resolvers.

**schema.gql**

```js
    type Query {
        allCharacters: [Character]!
        allSpells: [Spell]!
    }

    type Character {
        name: String!
        house: String
        patronus: String
        bloodStatus: String
        role: String
        school: String
        deathEater: Boolean
        dumbledoresArmy: Boolean
        orderOfThePheonix: Boolean
        ministryOfMagic: Boolean
        alias: String
        wand: String
        boggart: String
        animagus: String
    }

    type Spell {
        effect: String
        spell: String
        type: String
    }
```

Our schema is defining the shape of the data that we will soon be seeding into
the data from the Potter API. Our top level query will return two things, an
array of Characters and an array of Spells. We have then defined our Character
and Spell types. We don't need to specify an id here as when we seed the data
from the Potter API we will attach it then.

Now that we have our schema we can import it into Fauna. Head to your fauna
console and navigate to the graphql tab on the left, click import schema and
find the file we just created, click import and prepare to be amazed!

Once the import is complete we will be presented with a graphql playground where
we can run queries against our newly created database using its schema. Alas, we
have yet to add any data, but you can check the collections and indexes tabs on
the left of the console and see that fauna has created two new collections for
us, Character and Spell.

A collection is a grouping of our data with each piece of data being a document.
Or a table with rows if you are coming from an SQL background. Click the indexes
tab to see our two new query indexes that we specified in our schema,
allCharacters and allSpells. db-connection.js

Inside db-connection.js we will create the Fauna client connection, we will use
this connection to seed data into our database.

```js
require('dotenv').config();
const faunadb = require('faunadb');
const query = faunadb.query;

function createClient() {
  if (!process.env.FAUNA_ADMIN) {
    throw new Error(
      `No FAUNA_ADMIN key in found, please check your fauna dashboard or create a new key.`,
    );
  }
  const client = new faunadb.Client({
    secret: process.env.FAUNA_ADMIN,
  });
  return client;
}
exports.client = createClient();
exports.query = query;
```

Here we are creating a function which will check to see if we have an admin key
from our Fauna database, if none is found we are returning a helpful error
message to the console. If the key is found we are creating a connection to our
Fauna database and exporting that connection from file. We are also exporting
the query variable from Fauna as that will allow us to use some
[FQL](https://docs.fauna.com/fauna/current/api/fql/) (Fauna Query Language) when
seeding our data.

Head over to your Fauna console and click the security tab, click new key and
select admin from the role dropdown. The admin role will allow us to manage the
database, in our case, seed data into it. Choose the name FAUNA_ADMIN and hit
save. We will need to create another key for use in using our stored schema from
Fauna. Select server for the role of this key and name it SERVER_KEY. Don't
forget to make a note of the keys before you close the windows as you wont be
able to view them again!

That’s a great start. Next up we will seed our data and begin implementing our
frontend!
